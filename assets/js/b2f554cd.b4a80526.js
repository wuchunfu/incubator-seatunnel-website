"use strict";(self.webpackChunkseatunnel_website=self.webpackChunkseatunnel_website||[]).push([[1477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"The practice of SeaTunnel in Vip","metadata":{"permalink":"/blog/The practice of SeaTunnel in Vip","editUrl":"https://github.com/apache/incubator-seatunnel-website/edit/main/blog/2022-2-18-Meetup-vip.md","source":"@site/blog/2022-2-18-Meetup-vip.md","title":"The practice of SeaTunnel in Vip","description":"Guest speaker: Vip Senior Big Data Engineer Wang Yu","date":"2022-02-18T00:00:00.000Z","formattedDate":"February 18, 2022","tags":[{"label":"Vip","permalink":"/blog/tags/vip"},{"label":"ClickHouse","permalink":"/blog/tags/click-house"}],"readingTime":12.41,"truncated":false,"authors":[],"frontMatter":{"slug":"The practice of SeaTunnel in Vip","title":"The practice of SeaTunnel in Vip","tags":["Vip","ClickHouse"]},"nextItem":{"title":"How to quickly import data from HDFS into ClickHouse","permalink":"/blog/hdfs-to-clickhouse"}},"content":"Guest speaker: Vip Senior Big Data Engineer Wang Yu\\r\\nLecture preparation: Zhang Detong\\r\\n\\r\\nIntroduction: Vip referenced SeaTunnel as early as version 1.0. We use SeaTunnel to perform some data interaction work between Hive and ClickHouse.\\r\\nToday\'s presentation will focus on the following points:\\r\\n\\r\\n* Requirements and pain points of ClickHouse data import;\\r\\n* Selection of ClickHouse warehousing and warehousing tools;\\r\\n* Hive to ClickHouse;\\r\\n* ClickHouse to Hive;\\r\\n* Integration of SeaTunnel and Vipshop data platform;\\r\\n* Future outlook;\\r\\n\\r\\n# Requirements and pain points of ClickHouse data import\\r\\n## 1. Vipshop Data OLAP Architecture\\r\\nThe picture shows the OLAP architecture of Vipshop. The modules we are responsible for are the data service and the computing engine in the picture. The underlying data warehouses are divided into offline data warehouses, real-time data warehouses, and lake warehouses. For computing engines, we use Presto, Kylin and Clickhouse. Although Clickhouse is a storage-integrated OLAP database, we have included it in the computing engine part in order to take advantage of Clickhouse\'s excellent computing performance. Based on OLAP components, we provide SQL data services and non-SQL independent analysis of Vipshop to serve different intelligences. For example, non-SQL services are services that provide data analysis that is closer to the business for BI and commerce. Multiple data applications are abstracted on top of data services.\\r\\n![1](/doc/image_zh/2022-2-18-Meetup-vip/1-1.png)\\r\\n\\r\\n## 2. Requirements\\r\\nWe connect the underlying Hive, Kudu, and Alluxio components through Presto Connector and Spark components. Big data components can import and export data to and from each other, and you can use appropriate components to analyze data according to the needs and scenarios of data analysis. But when we introduced Clickhouse, it was a data island, and it was difficult to import and export data. There is a lot of work between Hive and Clickhouse to implement import and export. Our first data import and export requirement is to improve the import and export efficiency and incorporate Clickhouse into the big data system.\\r\\n![2](/doc/image_zh/2022-2-18-Meetup-vip/2.png)\\r\\n\\r\\nThe second requirement is that Presto runs SQL relatively slowly. The figure shows an example of slow SQL. The SQL where condition in the figure sets the date, time range, and specific filter conditions. This kind of SQL usage runs slowly because Presto uses partition granularity to push down. Even after optimization by other methods such as Hive\'s bucket table and bucketing, the return time is a few seconds, which cannot meet business requirements. In this case, we need to use Clickhouse for offline OLAP computing acceleration.\\r\\n![3](/doc/image_zh/2022-2-18-Meetup-vip/3.png)\\r\\n\\r\\nOur real-time data is written to Clickhouse through Kafka and Flink SQL. However, it is not enough to use real-time data for analysis. It is necessary to use the Hive dimension table and the T+1 real-time table with the ETL calculation number for accelerated transportation in Clickhouse. This requires importing Hive data into Clickhouse, which is our third requirement.\\r\\n![4](/doc/image_zh/2022-2-18-Meetup-vip/4.png)\\r\\n\\r\\n## 3. Pain points\\r\\nFirst, we introduce a data component to consider its performance. The granularity of the Hive table is five minutes. Is there a component that can support a short ETL process and import the ETL results into Clickhouse within five minutes? Second, we need to ensure the quality of the data, and the accuracy of the data needs to be guaranteed. The number of data entries in Hive and Clickhouse needs to be consistent. If there is a problem with the data quality, can the data be repaired by rerunning and other mechanisms? Third, are the data types that data import needs to support complete? The data types and some mechanisms between different databases are different. We have data types such as HiperLogLog and BitMap that are widely used in a certain storage engine. Whether they can be correctly transmitted and identified, and can be used well.\\r\\n\\r\\n# Selection of ClickHouse and Hive warehousing and warehousing tools\\r\\nBased on the pain points in the data business, we have compared and selected data warehouse and warehouse tools. We mainly choose among open source tools, without considering commercial warehouse entry and exit tools, we mainly compare DataX, SeaTunnel, and write Spark programs and use jdbc to insert ClickHouse among the three options.\\r\\nSeaTunnel and Spark rely on Vipshop\'s own Yarn cluster, which can directly implement distributed reading and writing. DataX is non-distributed, and the startup process between Reader and Writer takes a long time, and the performance is ordinary. The performance of SeaTunnel and Spark for data processing can reach several times that of DataX.\\r\\nData of more than one billion can run smoothly in SeaTunnel and Spark. DataX has great performance pressure after the amount of data is large, and it is difficult to process data of more than one billion.\\r\\nIn terms of read and write plug-in scalability, SeaTunnel supports a variety of data sources and supports users to develop plug-ins. SeaTunnel supports data import into Redis.\\r\\nIn terms of stability, since SeaTunnel and DataX are self-contained tools, the stability will be better. The stability aspect of Spark requires attention to code quality.\\r\\n![5](/doc/image_zh/2022-2-18-Meetup-vip/5.png)\\r\\n\\r\\nThe amount of data in our exposure table is in the billions of levels every day. We have the performance requirement to complete data processing within 5 minutes. We have the need to import and export data to Redis. We need import and export tools that can be connected to the data platform for task scheduling. . For the consideration of data volume, performance, scalability, and platform compatibility, we chose SeaTunnel as our data warehouse import and export tool.\\r\\n# Import Hive data into ClickHouse\\r\\nThe following will introduce our use of SeaTunnel.\\r\\nThe picture is a Hive table, which is our three-level product dimension table, including category products, dimension categories, and user population information. The primary key of the table is a third-level category ct_third_id, and the following value is the bitmap of two uids, which is the bitmap type of the user id. We need to import this Hive table into Clickhouse.\\r\\n![6](/doc/image_zh/2022-2-18-Meetup-vip/6.png)\\r\\n\\r\\nSeaTunnel is easy to install, and the official website documentation describes how to install it. The figure below shows the configuration of SeaTunnel. In the configuration, env, source and sink are essential. In the env part, the example in the figure is the Spark configuration. The configuration includes concurrency, etc. These parameters can be adjusted. The source part is the data source. The Hive data source is configured here, including a Hive Select statement. Spark runs the SQL in the source configuration to read the data. UDF is supported here for simple ETL; the sink part is configured with Clickhouse, and you can see output_type= rowbinary and rowbinary are the self-developed acceleration solutions of Vipshop; pre_sql and check_sql are self-developed functions for data verification, which will be described in detail later; clickhouse.socket_timeout and bulk_size can be adjusted according to the actual situation.\\r\\n![7](/doc/image_zh/2022-2-18-Meetup-vip/7.png)\\r\\n\\r\\nRun SeaTunnel, execute the sh script file, configure the conf file address and yarn information, and then you can.\\r\\n![8](/doc/image_zh/2022-2-18-Meetup-vip/8.png)\\r\\nSpark logs are generated during the running process, and both successful running and running errors can be viewed in the logs.\\r\\n![9](/doc/image_zh/2022-2-18-Meetup-vip/9.png)\\r\\n\\r\\nIn order to better fit the business, Vipshop will make some improvements to SeaTunnel. All our ETL tasks need to be rerun. We support pre_sql and check_sql to implement data rerun and logarithm. The main process is to execute pre_sql for preprocessing after the data is ready, delete the old partition data in Clickhouse, store it in a directory, and restore the partition and rename when it fails. check_sql will check, and the whole process will end after the check is passed; if the check fails, it will be rerun according to the configuration, and if the rerun fails, the corresponding person in charge will be alerted.\\r\\n![10](/doc/image_zh/2022-2-18-Meetup-vip/10.png)\\r\\n\\r\\n\\r\\nBased on the 1.0 version of SeaTunnel, Vipshop has added RowBinary for acceleration, and it also makes it easier to import the binary files of HuperLogLog and BinaryBitmap from Hive to Clickhouse. We made changes in ClickHouse-jdbc, bulk_size, Hive-source. Use the extended api of CK-jdbc to write data to CK in rowbinary mode. Bulk_size introduces the control logic for writing to CK in rowbinary mode. Hive-source\\r\\nRDD is partitioned with HashPartitioner to break up data to prevent data from being skewed.\\r\\n\\r\\nWe also let SeaTunnel support multiple types. In order to circle the crowd, corresponding methods need to be implemented in Clickhouse, Preso, and Spark. We added Callback, HttpEntity, and RowBinaryStream that support Batch feature to Clickhouse-jdbc, added bitmap type mapping to Clickhouse-jdbc and Clickhouse-sink code, and implemented UDF of Clickhouse\'s Hyperloglog and Bitmap functions in Presto and Spark.\\r\\nIn the previous configuration, the clickhouse-sink part can specify the table name, and here is the difference between writing to the local table and the distributed table. The performance of writing to a distributed table is worse than that of writing to a local table, which will put more pressure on the Clickhouse cluster. However, in scenarios such as exposure meter, flow meter, and ABTest, two tables are required to join, and both tables are in the order of billions. . At this time, we hope that the Join key falls on the local machine, and the Join cost is smaller. When we built the table, we configured the murmurHash64 rule in the distributed table distribution rules of Clickhouse, and then directly configured the distributed table in the sink of Seatunnel, handed the writing rules to Clickhouse, and used the characteristics of the distributed table to write. Writing to the local table will put less pressure on Clickhouse, and the performance of writing will be better. In Seatunnel, we go to Clickhouse\'s System.cluster table to obtain the table distribution information and machine distribution host according to the sink\'s local table. Then write to these hosts according to the equalization rule. Put the distributed writing of data into Seatunnel.\\r\\nFor the writing of local tables and distributed tables, our future transformation direction is to implement consistent hashing in Seatunnel, write directly according to certain rules, such as Clickhouse, without relying on Clickhouse itself for data distribution, and improve Clickhouse\'s high CPU load problem.\\r\\n\\r\\n# ClickHouse data import into Hive\\r\\nWe have the needs of people in the circle. Every day, Vipshop gathers 200,000 people in the supplier circle, such as people born in the 1980s, Gao Fushuai, and Bai Fumei. These Bitmap crowd information in Clickhouse needs to be exported to the Hive table, coordinated with other ETL tasks in Hive, and finally pushed to PIKA for use by external media. We made SeaTunnel back-push Clickhouse Bitmap crowd data to Hive.\\r\\n![11](/doc/image_zh/2022-2-18-Meetup-vip/11.png)\\r\\n\\r\\nThe figure shows the SeaTunnel configuration. We configure the source as Clickhouse, the sink as Hive, and the data verification is also configured in Hive.\\r\\n![12](/doc/image_zh/2022-2-18-Meetup-vip/12.png)\\r\\n\\r\\nSince we access SeaTunnel earlier, we have processed some modules, including adding plugin-spark-sink-hive module, plugin-spark-source-ClickHouse module, and rewriting Spark Row related methods so that they can be packaged through The Clickhouse data mapped by Schem, reconstruct the StructField and generate the DataFrame that finally needs to land on Hive. The latest version has many source and sink components, which is more convenient to use in SeaTunnel. It is now also possible to integrate the Flink connector directly in SeaTunnel.\\r\\n\\r\\n# Integration of SeaTunnel and Vipshop Data Platform\\r\\nEach company has its own scheduling system, such as Beluga, Zeus. The scheduling tool of Vipshop is Shufang, and the scheduling tool integrates the data transmission tool. The following is the architecture diagram of the scheduling system, which includes the entry and exit of various types of data.\\r\\n![13](/doc/image_zh/2022-2-18-Meetup-vip/13.png)\\r\\n\\r\\nThe SeaTunnel task type is integrated into the platform. The picture is a screenshot of the scheduled task of Shufang. You can see that the selected part is a configured SeaTunnel task. resource information. The following shows the historical running instance information.\\r\\n![14](/doc/image_zh/2022-2-18-Meetup-vip/14.png)\\r\\n\\r\\nWe integrated SeaTunnel into the scheduling system. The Shufang Scheduling Master will assign tasks to the corresponding Agents according to the task types, and assign them to the appropriate machines to run according to the Agent load. The controller pulls the task scheduling configuration and information in the foreground. After arriving, a SeaTunnel cluster is generated and executed in a virtual environment similar to k8s pod and cgroup isolation. The running result will be judged by the data quality monitoring of the scheduling platform whether the task is completed and whether the operation is successful, and if it fails, it will rerun and alarm.\\r\\n![15](/doc/image_zh/2022-2-18-Meetup-vip/15.png)\\r\\n\\r\\nSeaTunnel itself is a tool-based component, which is used to manage and control data blood relationship, data quality, historical records, high-alert monitoring, and resource allocation. We integrate SeaTunnel into the platform, and we can take advantage of the platform to take advantage of SeaTunnel.\\r\\nSeaTunnel is used for processing in the deposit crowd. By managing data, we divide the circled people into different people according to their paths and usage conditions, or thousands of people and thousands of faces, tag users, and push a certain type of people circled to users, analysts and suppliers.\\r\\n![16](/doc/image_zh/2022-2-18-Meetup-vip/16.png)\\r\\n\\r\\nThe traffic enters Kafka, enters the warehouse through Flink, and then forms a user label table through ETL. After the user label table is generated, we use the BitMap method implemented by Presto to type the data into a wide table in Hive. Users create tasks by box-selecting entries on the crowd system page, submit to Tengqun, and generate SQL query Clickhouse BitMap. Clickhouse\'s BitMap query speed is very fast. Due to its inherent advantages, we need to import Hive\'s BitMap table into Clickhouse through SeaTunnel. After the crowd is circled, we need to land the table to form a partition or a record of Clickhouse, and then store the resulting BitMap table in Hive through SeaTunnel. Finally, the synchronization tool will synchronize Hive\'s BitMap crowd results to the external media repository Pika. Around 20w people are circled every day.\\r\\nDuring the whole process, SeaTunnel is responsible for exporting the data from Hive to Clickhouse. After the ETL process of Clickhouse is completed, SeaTunnel exports the data from Clickhouse to Hive.\\r\\nIn order to fulfill this requirement, we implemented UDFs of ClickHouse\'s Hyperloglog and BitMap functions on Presto and Spark; we also developed the Seatunnel interface, so that the crowds circled by users using the Bitmap method in ClickHouse can be directly written to the Hive table through Seatunnel , without an intermediate landing step. Users can also call the SeaTunnel interface through spark to circle the crowd or reverse the crowd bitmap in Hive, so that the data can be directly transmitted to the ClickHouse result table without intermediate landing.\\r\\n# Follow-up\\r\\nIn the future, we will further improve the problem of high CPU load when Clickhouse writes data. In the next step, we will implement the CK-local mode of Clickhouse data source and read end in SeaTunnel, separate read and write, and reduce Clickhouse pressure. In the future we will also add more sink support, such as data push to Pika and corresponding data checking."},{"id":"hdfs-to-clickhouse","metadata":{"permalink":"/blog/hdfs-to-clickhouse","editUrl":"https://github.com/apache/incubator-seatunnel-website/edit/main/blog/2021-12-30-hdfs-to-clickhouse.md","source":"@site/blog/2021-12-30-hdfs-to-clickhouse.md","title":"How to quickly import data from HDFS into ClickHouse","description":"ClickHouse is a distributed columnar DBMS for OLAP. Our department has now stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion.","date":"2021-12-30T00:00:00.000Z","formattedDate":"December 30, 2021","tags":[{"label":"HDFS","permalink":"/blog/tags/hdfs"},{"label":"ClickHouse","permalink":"/blog/tags/click-house"}],"readingTime":5.455,"truncated":false,"authors":[],"frontMatter":{"slug":"hdfs-to-clickhouse","title":"How to quickly import data from HDFS into ClickHouse","tags":["HDFS","ClickHouse"]},"prevItem":{"title":"The practice of SeaTunnel in Vip","permalink":"/blog/The practice of SeaTunnel in Vip"},"nextItem":{"title":"How to quickly import data from Hive into ClickHouse","permalink":"/blog/hive-to-clickhouse"}},"content":"ClickHouse is a distributed columnar DBMS for OLAP. Our department has now stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion.\\n\\nThe experience of data processing and storage introduced earlier is based on real-time data streams. The data is stored in Kafka. We use Java or Golang to read, parse, and clean the data from Kafka and write it into ClickHouse, so that the data can be stored in ClickHouse. Quick access. However, in the usage scenarios of many students, the data is not real-time, and it may be necessary to import the data in HDFS or Hive into ClickHouse. Some students implement data import by writing Spark programs, so is there a simpler and more efficient way?\\n\\nAt present, there is a tool **Seatunnel** in the open source community, the project address [https://github.com/apache/incubator-seatunnel](https://github.com/apache/incubator-seatunnel), can quickly Data in HDFS is imported into ClickHouse.\\n\\n## HDFS To ClickHouse\\n\\nAssuming that our logs are stored in HDFS, we need to parse the logs and filter out the fields we care about, and write the corresponding fields into the ClickHouse table.\\n\\n### Log Sample\\n\\nThe log format we store in HDFS is as follows, which is a very common Nginx log\\n\\n```shell\\n10.41.1.28 github.com 114.250.140.241 0.001s \\"127.0.0.1:80\\" [26/Oct/2018:03:09:32 +0800] \\"GET /Apache/Seatunnel HTTP/1.1\\" 200 0 \\"-\\" - \\"Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)\\" \\"196\\" \\"-\\" \\"mainpage\\" \\"443\\" \\"-\\" \\"172.16.181.129\\"\\n```\\n\\n### ClickHouse Schema\\n\\nOur ClickHouse table creation statement is as follows, our table is partitioned by day\\n\\n```shell\\nCREATE TABLE cms.cms_msg\\n(\\n    date Date, \\n    datetime DateTime, \\n    url String, \\n    request_time Float32, \\n    status String, \\n    hostname String, \\n    domain String, \\n    remote_addr String, \\n    data_size Int32, \\n    pool String\\n) ENGINE = MergeTree PARTITION BY date ORDER BY date SETTINGS index_granularity = 16384\\n```\\n\\n## Seatunnel with ClickHouse\\n\\nNext, I will introduce to you in detail how we can meet the above requirements through Seatunnel and write the data in HDFS into ClickHouse.\\n\\n### Seatunnel\\n\\n[Seatunnel](https://github.com/apache/incubator-seatunnel) is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plugins that support reading data from Kafka, HDFS, Kudu, performing various data processing, and writing the results to ClickHouse, Elasticsearch or Kafka.\\n\\n### Prerequisites\\n\\nFirst we need to install Seatunnel, the installation is very simple, no need to configure system environment variables\\n\\n1. Prepare the Spark environment\\n2. Install Seatunnel\\n3. Configure Seatunnel\\n\\nThe following are simple steps, the specific installation can refer to [Quick Start](/docs/quick-start)\\n\\n```shell\\ncd /usr/local\\n\\nwget https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\ntar -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\n\\nwget https://github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel-1.1.1.zip\\n\\nunzip seatunnel-1.1.1.zip\\n\\ncd seatunnel-1.1.1\\nvim config/seatunnel-env.sh\\n\\n# Specify the Spark installation path\\nSPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.2.0-bin-hadoop2.7}\\n```\\n\\n### seatunnel Pipeline\\n\\nWe only need to write a configuration file of seatunnel Pipeline to complete the data import.\\n\\nThe configuration file consists of four parts, Spark, Input, filter and Output.\\n\\n#### Spark\\n\\nThis part is the related configuration of Spark, which mainly configures the size of the resources required for Spark to execute.\\n\\n```shell\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n```\\n\\n#### Input\\n\\nThis part defines the data source. The following is a configuration example for reading data in text format from HDFS files.\\n\\n```shell\\ninput {\\n    hdfs {\\n        path = \\"hdfs://nomanode:8020/rowlog/accesslog\\"\\n        table_name = \\"access_log\\"\\n        format = \\"text\\"\\n    }\\n}\\n```\\n\\n#### Filter\\n\\nIn the Filter section, here we configure a series of transformations, including regular parsing to split the log, time transformation to convert HTTPDATE to the date format supported by ClickHouse, type conversion to Number type fields, and field filtering through SQL, etc.\\n\\n```shell\\nfilter {\\n    # Parse raw logs using regular expressions\\n    grok {\\n        source_field = \\"raw_message\\"\\n        pattern = \'%{IP:ha_ip}\\\\\\\\s%{NOTSPACE:domain}\\\\\\\\s%{IP:remote_addr}\\\\\\\\s%{NUMBER:request_time}s\\\\\\\\s\\\\\\"%{DATA:upstream_ip}\\\\\\"\\\\\\\\s\\\\\\\\[%{HTTPDATE:timestamp}\\\\\\\\]\\\\\\\\s\\\\\\"%{NOTSPACE:method}\\\\\\\\s%{DATA:url}\\\\\\\\s%{NOTSPACE:http_ver}\\\\\\"\\\\\\\\s%{NUMBER:status}\\\\\\\\s%{NUMBER:body_bytes_send}\\\\\\\\s%{DATA:referer}\\\\\\\\s%{NOTSPACE:cookie_info}\\\\\\\\s\\\\\\"%{DATA:user_agent}\\\\\\"\\\\\\\\s%{DATA:uid}\\\\\\\\s%{DATA:session_id}\\\\\\\\s\\\\\\"%{DATA:pool}\\\\\\"\\\\\\\\s\\\\\\"%{DATA:tag2}\\\\\\"\\\\\\\\s%{DATA:tag3}\\\\\\\\s%{DATA:tag4}\'\\n    }\\n\\n    # Convert data in \\"dd/MMM/yyyy:HH:mm:ss Z\\" format to\\n    # Data in \\"yyyy/MM/dd HH:mm:ss\\" format\\n    date {\\n        source_field = \\"timestamp\\"\\n        target_field = \\"datetime\\"\\n        source_time_format = \\"dd/MMM/yyyy:HH:mm:ss Z\\"\\n        target_time_format = \\"yyyy/MM/dd HH:mm:ss\\"\\n    }\\n\\n    # Use SQL to filter the fields of interest and process the fields\\n    # You can even filter out data you don\'t care about by filter conditions\\n    sql {\\n        table_name = \\"access\\"\\n        sql = \\"select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access\\"\\n    }\\n}\\n```\\n\\n#### Output\\n\\nFinally, we write the processed structured data to ClickHouse\\n\\n```shell\\noutput {\\n    clickhouse {\\n        host = \\"your.clickhouse.host:8123\\"\\n        database = \\"seatunnel\\"\\n        table = \\"access_log\\"\\n        fields = [\\"date\\", \\"datetime\\", \\"hostname\\", \\"uri\\", \\"http_code\\", \\"request_time\\", \\"data_size\\", \\"domain\\"]\\n        username = \\"username\\"\\n        password = \\"password\\"\\n    }\\n}\\n```\\n\\n### Running seatunnel\\n\\nWe combine the above four-part configuration into our configuration file `config/batch.conf`.\\n\\n```shell\\nvim config/batch.conf\\n```\\n\\n```shell\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n\\ninput {\\n    hdfs {\\n        path = \\"hdfs://nomanode:8020/rowlog/accesslog\\"\\n        table_name = \\"access_log\\"\\n        format = \\"text\\"\\n    }\\n}\\n\\nfilter {\\n    # Parse raw logs using regular expressions\\n    grok {\\n        source_field = \\"raw_message\\"\\n        pattern = \'%{IP:ha_ip}\\\\\\\\s%{NOTSPACE:domain}\\\\\\\\s%{IP:remote_addr}\\\\\\\\s%{NUMBER:request_time}s\\\\\\\\s\\\\\\"%{DATA:upstream_ip}\\\\\\"\\\\\\\\s\\\\\\\\[%{HTTPDATE:timestamp}\\\\\\\\]\\\\\\\\s\\\\\\"%{NOTSPACE:method}\\\\\\\\s%{DATA:url}\\\\\\\\s%{NOTSPACE:http_ver}\\\\\\"\\\\\\\\s%{NUMBER:status}\\\\\\\\s%{NUMBER:body_bytes_send}\\\\\\\\s%{DATA:referer}\\\\\\\\s%{NOTSPACE:cookie_info}\\\\\\\\s\\\\\\"%{DATA:user_agent}\\\\\\"\\\\\\\\s%{DATA:uid}\\\\\\\\s%{DATA:session_id}\\\\\\\\s\\\\\\"%{DATA:pool}\\\\\\"\\\\\\\\s\\\\\\"%{DATA:tag2}\\\\\\"\\\\\\\\s%{DATA:tag3}\\\\\\\\s%{DATA:tag4}\'\\n    }\\n\\n    # Convert data in \\"dd/MMM/yyyy:HH:mm:ss Z\\" format to\\n    # Data in \\"yyyy/MM/dd HH:mm:ss\\" format\\n    date {\\n        source_field = \\"timestamp\\"\\n        target_field = \\"datetime\\"\\n        source_time_format = \\"dd/MMM/yyyy:HH:mm:ss Z\\"\\n        target_time_format = \\"yyyy/MM/dd HH:mm:ss\\"\\n    }\\n\\n    # Use SQL to filter the fields of interest and process the fields\\n    # You can even filter out data you don\'t care about by filter conditions\\n    sql {\\n        table_name = \\"access\\"\\n        sql = \\"select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access\\"\\n    }\\n}\\n\\noutput {\\n    clickhouse {\\n        host = \\"your.clickhouse.host:8123\\"\\n        database = \\"seatunnel\\"\\n        table = \\"access_log\\"\\n        fields = [\\"date\\", \\"datetime\\", \\"hostname\\", \\"uri\\", \\"http_code\\", \\"request_time\\", \\"data_size\\", \\"domain\\"]\\n        username = \\"username\\"\\n        password = \\"password\\"\\n    }\\n}\\n```\\n\\nExecute the command, specify the configuration file, and run Seatunnel to write data to ClickHouse. Here we take the local mode as an example.\\n\\n```shell\\n./bin/start-seatunnel.sh --config config/batch.conf -e client -m \'local[2]\'\\n```\\n\\n## Conclusion\\n\\nIn this post, we covered how to import Nginx log files from HDFS into ClickHouse using Seatunnel. Data can be imported quickly with only one configuration file without writing any code. In addition to supporting HDFS data sources, Seatunnel also supports real-time reading and processing of data from Kafka to ClickHouse. Our next article will describe how to quickly import data from Hive into ClickHouse.\\n\\nOf course, Seatunnel is not only a tool for ClickHouse data writing, but also plays a very important role in the writing of data sources such as Elasticsearch and Kafka.\\n\\nIf you want to know more functions and cases of Seatunnel combined with ClickHouse, Elasticsearch and Kafka, you can go directly to the official website [https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n\\n-- Power by [InterestingLab](https://github.com/InterestingLab)"},{"id":"hive-to-clickhouse","metadata":{"permalink":"/blog/hive-to-clickhouse","editUrl":"https://github.com/apache/incubator-seatunnel-website/edit/main/blog/2021-12-30-hive-to-clickhouse.md","source":"@site/blog/2021-12-30-hive-to-clickhouse.md","title":"How to quickly import data from Hive into ClickHouse","description":"ClickHouse is a distributed columnar DBMS for OLAP. Our department has stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion.","date":"2021-12-30T00:00:00.000Z","formattedDate":"December 30, 2021","tags":[{"label":"Hive","permalink":"/blog/tags/hive"},{"label":"ClickHouse","permalink":"/blog/tags/click-house"}],"readingTime":4.215,"truncated":false,"authors":[],"frontMatter":{"slug":"hive-to-clickhouse","title":"How to quickly import data from Hive into ClickHouse","tags":["Hive","ClickHouse"]},"prevItem":{"title":"How to quickly import data from HDFS into ClickHouse","permalink":"/blog/hdfs-to-clickhouse"},"nextItem":{"title":"How to quickly write data to Elasticsearch using Spark","permalink":"/blog/spark-execute-elasticsearch"}},"content":"ClickHouse is a distributed columnar DBMS for OLAP. Our department has stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion.\\n\\nIn the previous article [How to quickly import data from HDFS into ClickHouse] (2021-12-30-hdfs-to-clickhouse.md), we mentioned the use of Seatunnel [https://github.com/apache/incubator -seatunnel](https://github.com/apache/incubator-seatunnel) After a very simple operation on the data in HDFS, the data can be written to ClickHouse. The data in HDFS is generally unstructured data, so what should we do with the structured data stored in Hive?\\n\\n![](/doc/image_zh/hive-logo.png)\\n\\n## Hive to ClickHouse\\n\\nAssuming that our data has been stored in Hive, we need to read the data in the Hive table and filter out the fields we care about, or convert the fields, and finally write the corresponding fields into the ClickHouse table.\\n\\n### Hive Schema\\n\\nThe structure of the data table we store in Hive is as follows, which stores common Nginx logs.\\n\\n```\\nCREATE TABLE `nginx_msg_detail`(\\n   `hostname` string,\\n   `domain` string,\\n   `remote_addr` string,\\n   `request_time` float,\\n   `datetime` string,\\n   `url` string,\\n   `status` int,\\n   `data_size` int,\\n   `referer` string,\\n   `cookie_info` string,\\n   `user_agent` string,\\n   `minute` string)\\n PARTITIONED BY (\\n   `date` string,\\n   `hour` string)\\n\\n```\\n\\n### ClickHouse Schema\\n\\nOur ClickHouse table creation statement is as follows, our table is partitioned by day\\n\\n```\\nCREATE TABLE cms.cms_msg\\n(\\n    date Date,\\n    datetime DateTime,\\n    url String,\\n    request_time Float32,\\n    status String,\\n    hostname String,\\n    domain String,\\n    remote_addr String,\\n    data_size Int32\\n) ENGINE = MergeTree PARTITION BY date ORDER BY (date, hostname) SETTINGS index_granularity = 16384\\n```\\n\\n## Seatunnel with ClickHouse\\n\\nNext, I will introduce to you how we write data from Hive to ClickHouse through Seatunnel.\\n\\n### Seatunnel\\n\\n[Seatunnel](https://github.com/apache/incubator-seatunnel) is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plug-ins that support reading data from Kafka, HDFS, and Kudu, performing various data processing, and writing the results to ClickHouse, Elasticsearch or Kafka.\\n\\nThe environment preparation and installation steps of Seatunnel will not be repeated here. For specific installation steps, please refer to the previous article or visit [Seatunnel Docs](/docs/introduction)\\n\\n### Seatunnel Pipeline\\n\\nWe only need to write a configuration file of Seatunnel Pipeline to complete the data import.\\n\\nThe configuration file includes four parts, namely Spark, Input, filter and Output.\\n\\n#### Spark\\n\\n\\nThis part is the related configuration of Spark, which mainly configures the resource size required for Spark execution.\\n\\n```\\nspark {\\n  // This configuration is required\\n  spark.sql.catalogImplementation = \\"hive\\"\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n```\\n\\n#### Input\\n\\nThis part defines the data source. The following is a configuration example of reading data in text format from a Hive file.\\n\\n```\\ninput {\\n    hive {\\n        pre_sql = \\"select * from access.nginx_msg_detail\\"\\n        table_name = \\"access_log\\"\\n    }\\n}\\n```\\n\\nSee, a very simple configuration can read data from Hive. `pre_sql` is the SQL to read data from Hive, and `table_name` is the name of the table that will register the read data as a temporary table in Spark, which can be any field.\\n\\nIt should be noted that it must be ensured that the metastore of hive is in the service state.\\n\\nWhen running in Cluster, Client, Local mode, the `hive-site.xml` file must be placed in the $HADOOP_CONF directory of the submit task node\\n\\n#### Filter\\n\\nIn the Filter section, here we configure a series of transformations, and here we discard the unnecessary minute and hour fields. Of course, we can also not read these fields through `pre_sql` when reading Hive\\n\\n```\\nfilter {\\n    remove {\\n        source_field = [\\"minute\\", \\"hour\\"]\\n    }\\n}\\n```\\n\\n#### Output\\n\\nFinally, we write the processed structured data to ClickHouse\\n\\n```\\noutput {\\n    clickhouse {\\n        host = \\"your.clickhouse.host:8123\\"\\n        database = \\"seatunnel\\"\\n        table = \\"nginx_log\\"\\n        fields = [\\"date\\", \\"datetime\\", \\"hostname\\", \\"url\\", \\"http_code\\", \\"request_time\\", \\"data_size\\", \\"domain\\"]\\n        username = \\"username\\"\\n        password = \\"password\\"\\n    }\\n}\\n```\\n\\n### Running Seatunnel\\n\\nWe combine the above four-part configuration into our configuration file `config/batch.conf`.\\n\\n    vim config/batch.conf\\n\\n```\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n  // This configuration is required\\n  spark.sql.catalogImplementation = \\"hive\\"\\n}\\ninput {\\n    hive {\\n        pre_sql = \\"select * from access.nginx_msg_detail\\"\\n        table_name = \\"access_log\\"\\n    }\\n}\\nfilter {\\n    remove {\\n        source_field = [\\"minute\\", \\"hour\\"]\\n    }\\n}\\noutput {\\n    clickhouse {\\n        host = \\"your.clickhouse.host:8123\\"\\n        database = \\"seatunnel\\"\\n        table = \\"access_log\\"\\n        fields = [\\"date\\", \\"datetime\\", \\"hostname\\", \\"uri\\", \\"http_code\\", \\"request_time\\", \\"data_size\\", \\"domain\\"]\\n        username = \\"username\\"\\n        password = \\"password\\"\\n    }\\n}\\n```\\n\\nExecute the command, specify the configuration file, and run Seatunnel to write data to ClickHouse. Here we take the local mode as an example.\\n\\n    ./bin/start-seatunnel.sh --config config/batch.conf -e client -m \'local[2]\'\\n\\n\\n## Conclusion\\n\\nIn this post, we covered how to import data from Hive into ClickHouse using Seatunnel. The data import can be completed quickly through only one configuration file without writing any code, which is very simple.\\n\\nIf you want to know more functions and cases of Seatunnel combined with ClickHouse, Elasticsearch, Kafka, Hadoop, you can go directly to the official website [https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n\\n-- Power by [InterestingLab](https://github.com/InterestingLab)"},{"id":"spark-execute-elasticsearch","metadata":{"permalink":"/blog/spark-execute-elasticsearch","editUrl":"https://github.com/apache/incubator-seatunnel-website/edit/main/blog/2021-12-30-spark-execute-elasticsearch.md","source":"@site/blog/2021-12-30-spark-execute-elasticsearch.md","title":"How to quickly write data to Elasticsearch using Spark","description":"When it comes to writing data to Elasticsearch, the first thing that comes to mind must be Logstash. Logstash is accepted by the majority of users because of its simplicity, scalability, and scalability. However, the ruler is shorter and the inch is longer, and Logstash must have application scenarios that it cannot apply to, such as:","date":"2021-12-30T00:00:00.000Z","formattedDate":"December 30, 2021","tags":[{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"Kafka","permalink":"/blog/tags/kafka"},{"label":"Elasticsearch","permalink":"/blog/tags/elasticsearch"}],"readingTime":5.725,"truncated":false,"authors":[],"frontMatter":{"slug":"spark-execute-elasticsearch","title":"How to quickly write data to Elasticsearch using Spark","tags":["Spark","Kafka","Elasticsearch"]},"prevItem":{"title":"How to quickly import data from Hive into ClickHouse","permalink":"/blog/hive-to-clickhouse"},"nextItem":{"title":"How to use Spark to do OLAP analysis on TiDB","permalink":"/blog/spark-execute-tidb"}},"content":"When it comes to writing data to Elasticsearch, the first thing that comes to mind must be Logstash. Logstash is accepted by the majority of users because of its simplicity, scalability, and scalability. However, the ruler is shorter and the inch is longer, and Logstash must have application scenarios that it cannot apply to, such as:\\n\\n* Massive data ETL\\n* Massive data aggregation\\n* Multi-source data processing\\n\\nIn order to meet these scenarios, many students will choose Spark, use Spark operators to process data, and finally write the processing results to Elasticsearch.\\n\\nOur department used Spark to analyze Nginx logs, counted our web service access, aggregated Nginx logs every minute and finally wrote the results to Elasticsearch, and then used Kibana to configure real-time monitoring of the Dashboard. Both Elasticsearch and Kibana are convenient and practical, but with more and more similar requirements, how to quickly write data to Elasticsearch through Spark has become a big problem for us.\\n\\nToday, I would like to recommend a black technology Seatunnel [https://github.com/apache/incubator-seatunnel](https://github.com/apache/incubator-seatunnel) that can realize fast data writing. It is very easy to use , a high-performance, real-time data processing product that can deal with massive data. It is built on Spark and is easy to use, flexibly configured, and requires no development.\\n\\n![](/doc/image_zh/wd-struct.png)\\n\\n\\n## Kafka to Elasticsearch\\n\\nLike Logstash, Seatunnel also supports multiple types of data input. Here we take the most common Kakfa as the input source as an example to explain how to use Seatunnel to quickly write data to Elasticsearch\\n\\n### Log Sample\\n\\nThe original log format is as follows:\\n```\\n127.0.0.1 elasticsearch.cn 114.250.140.241 0.001s \\"127.0.0.1:80\\" [26/Oct/2018:21:54:32 +0800] \\"GET /article HTTP/1.1\\" 200 123 \\"-\\" - \\"Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)\\"\\n```\\n\\n### Elasticsearch Document\\n\\nWe want to count the visits of each domain name in one minute. The aggregated data has the following fields:\\n```\\ndomain String\\nhostname String\\nstatus int\\ndatetime String\\ncount int\\n```\\n\\n## Seatunnel with Elasticsearch\\n\\nNext, I will introduce you in detail, how we read the data in Kafka through Seatunnel, parse and aggregate the data, and finally write the processing results into Elasticsearch.\\n\\n### Seatunnel\\n\\n[Seatunnel](https://github.com/apache/incubator-seatunnel) also has a very rich plug-in that supports reading data from Kafka, HDFS, Hive, performing various data processing, and converting the results Write to Elasticsearch, Kudu or Kafka.\\n\\n### Prerequisites\\n\\nFirst of all, we need to install seatunnel, the installation is very simple, no need to configure system environment variables\\n1. Prepare the Spark environment\\n2. Install Seatunnel\\n3. Configure Seatunnel\\n\\nThe following are simple steps, the specific installation can refer to [Quick Start](/docs/quick-start)\\n\\n```yaml\\ncd /usr/local\\nwget https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\ntar -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\nwget https://github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel-1.1.1.zip\\nunzip seatunnel-1.1.1.zip\\ncd seatunnel-1.1.1\\n\\nvim config/seatunnel-env.sh\\n# Specify the Spark installation path\\nSPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.2.0-bin-hadoop2.7}\\n```\\n\\n### Seatunnel Pipeline\\n\\nLike Logstash, we only need to write a configuration file of Seatunnel Pipeline to complete the data import. I believe that friends who know Logstash can start Seatunnel configuration soon.\\n\\nThe configuration file includes four parts, namely Spark, Input, filter and Output.\\n\\n#### Spark\\n\\n\\nThis part is the related configuration of Spark, which mainly configures the resource size required for Spark execution.\\n```\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n  spark.streaming.batchDuration = 5\\n}\\n```\\n\\n#### Input\\n\\nThis part defines the data source. The following is a configuration example of reading data from Kafka,\\n\\n```\\nkafkaStream {\\n    topics = \\"seatunnel-es\\"\\n    consumer.bootstrap.servers = \\"localhost:9092\\"\\n    consumer.group.id = \\"seatunnel_es_group\\"\\n    consumer.rebalance.max.retries = 100\\n}\\n```\\n\\n#### Filter\\n\\nIn the Filter section, here we configure a series of conversions, including regular parsing to split logs, time conversion to convert HTTPDATE to a date format supported by Elasticsearch, type conversion for fields of type Number, and data aggregation through SQL\\n```yaml\\nfilter {\\n    # Parse the original log using regex\\n    # The initial data is in the raw_message field\\n    grok {\\n        source_field = \\"raw_message\\"\\n        pattern = \'%{NOTSPACE:hostname}\\\\\\\\s%{NOTSPACE:domain}\\\\\\\\s%{IP:remote_addr}\\\\\\\\s%{NUMBER:request_time}s\\\\\\\\s\\\\\\"%{DATA:upstream_ip}\\\\\\"\\\\\\\\s\\\\\\\\[%{HTTPDATE:timestamp}\\\\\\\\]\\\\\\\\s\\\\\\"%{NOTSPACE:method}\\\\\\\\s%{DATA:url}\\\\\\\\s%{NOTSPACE:http_ver}\\\\\\"\\\\\\\\s%{NUMBER:status}\\\\\\\\s%{NUMBER:body_bytes_send}\\\\\\\\s%{DATA:referer}\\\\\\\\s%{NOTSPACE:cookie_info}\\\\\\\\s\\\\\\"%{DATA:user_agent}\'\\n   }\\n    # Convert data in \\"dd/MMM/yyyy:HH:mm:ss Z\\" format to\\n    # format supported in Elasticsearch\\n    date {\\n        source_field = \\"timestamp\\"\\n        target_field = \\"datetime\\"\\n        source_time_format = \\"dd/MMM/yyyy:HH:mm:ss Z\\"\\n        target_time_format = \\"yyyy-MM-dd\'T\'HH:mm:ss.SSS+08:00\\"\\n    }\\n    ## Aggregate data with SQL\\n    sql {\\n        table_name = \\"access_log\\"\\n        sql = \\"select domain, hostname, int(status), datetime, count(*) from access_log group by domain, hostname, status, datetime\\"\\n    }\\n }\\n```\\n\\n#### Output\\nFinally, we write the processed structured data to Elasticsearch.\\n\\n```yaml\\noutput {\\n    elasticsearch {\\n        hosts = [\\"localhost:9200\\"]\\n        index = \\"seatunnel-${now}\\"\\n        es.batch.size.entries = 100000\\n        index_time_format = \\"yyyy.MM.dd\\"\\n    }\\n}\\n```\\n\\n### Running Seatunnel\\n\\nWe combine the above four-part configuration into our configuration file `config/batch.conf`.\\n\\n    vim config/batch.conf\\n\\n```\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n  spark.streaming.batchDuration = 5\\n}\\ninput {\\n    kafkaStream {\\n        topics = \\"seatunnel-es\\"\\n        consumer.bootstrap.servers = \\"localhost:9092\\"\\n        consumer.group.id = \\"seatunnel_es_group\\"\\n        consumer.rebalance.max.retries = 100\\n    }\\n}\\nfilter {\\n    # Parse the original log using regex\\n    # The initial data is in the raw_message field\\n    grok {\\n        source_field = \\"raw_message\\"\\n        pattern = \'%{IP:hostname}\\\\\\\\s%{NOTSPACE:domain}\\\\\\\\s%{IP:remote_addr}\\\\\\\\s%{NUMBER:request_time}s\\\\\\\\s\\\\\\"%{DATA:upstream_ip}\\\\\\"\\\\\\\\s\\\\\\\\[%{HTTPDATE:timestamp}\\\\\\\\]\\\\\\\\s\\\\\\"%{NOTSPACE:method}\\\\\\\\s%{DATA:url}\\\\\\\\s%{NOTSPACE:http_ver}\\\\\\"\\\\\\\\s%{NUMBER:status}\\\\\\\\s%{NUMBER:body_bytes_send}\\\\\\\\s%{DATA:referer}\\\\\\\\s%{NOTSPACE:cookie_info}\\\\\\\\s\\\\\\"%{DATA:user_agent}\'\\n   }\\n    # Convert data in \\"dd/MMM/yyyy:HH:mm:ss Z\\" format to\\n    # format supported in Elasticsearch\\n    date {\\n        source_field = \\"timestamp\\"\\n        target_field = \\"datetime\\"\\n        source_time_format = \\"dd/MMM/yyyy:HH:mm:ss Z\\"\\n        target_time_format = \\"yyyy-MM-dd\'T\'HH:mm:00.SSS+08:00\\"\\n    }\\n    ## Aggregate data with SQL\\n    sql {\\n        table_name = \\"access_log\\"\\n        sql = \\"select domain, hostname, status, datetime, count(*) from access_log group by domain, hostname, status, datetime\\"\\n    }\\n }\\noutput {\\n    elasticsearch {\\n        hosts = [\\"localhost:9200\\"]\\n        index = \\"seatunnel-${now}\\"\\n        es.batch.size.entries = 100000\\n        index_time_format = \\"yyyy.MM.dd\\"\\n    }\\n}\\n```\\n\\nExecute the command, specify the configuration file, and run Seatunnel to write data to Elasticsearch. Here we take the local mode as an example.\\n\\n    ./bin/start-seatunnel.sh --config config/batch.conf -e client -m \'local[2]\'\\n\\nFinally, the data written into Elasticsearch is as follows, and with Kibana, real-time monitoring of web services can be realized ^_^.\\n\\n```\\n\\"_source\\": {\\n    \\"domain\\": \\"elasticsearch.cn\\",\\n    \\"hostname\\": \\"localhost\\",\\n    \\"status\\": \\"200\\",\\n    \\"datetime\\": \\"2018-11-26T21:54:00.000+08:00\\",\\n    \\"count\\": 26\\n  }\\n```\\n\\n## Conclusion\\n\\nIn this post, we introduced how to write data from Kafka to Elasticsearch via Seatunnel. You can quickly run a Spark Application with only one configuration file, complete data processing and writing, and do not need to write any code, which is very simple.\\n\\nWhen there are scenarios that Logstash cannot support or the performance of Logstah cannot meet expectations during data processing, you can try to use Seatunnel to solve the problem.\\n\\nIf you want to know more functions and cases of using Seatunnel in combination with Elasticsearch, Kafka and Hadoop, you can go directly to the official website [https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n\\n\\n**We will publish another article \\"How to Use Spark and Elasticsearch for Interactive Data Analysis\\" in the near future, so stay tuned.**\\n\\n## Contract us\\n* Mailing list : **dev@seatunnel.apache.org**. Send anything to `dev-subscribe@seatunnel.apache.org` and subscribe to the mailing list according to the replies.\\n* Slack: Send a `Request to join SeaTunnel slack` email to the mailing list (`dev@seatunnel.apache.org`), and we will invite you to join (please make sure you are registered with Slack before doing so).\\n* [bilibili B station video](https://space.bilibili.com/1542095008)"},{"id":"spark-execute-tidb","metadata":{"permalink":"/blog/spark-execute-tidb","editUrl":"https://github.com/apache/incubator-seatunnel-website/edit/main/blog/2021-12-30-spark-execute-tidb.md","source":"@site/blog/2021-12-30-spark-execute-tidb.md","title":"How to use Spark to do OLAP analysis on TiDB","description":"TiDB is a fusion database product targeting online transaction processing/online analytical processing. Distributed transactions, real-time OLAP and other important features.","date":"2021-12-30T00:00:00.000Z","formattedDate":"December 30, 2021","tags":[{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"TiDB","permalink":"/blog/tags/ti-db"}],"readingTime":6.705,"truncated":false,"authors":[],"frontMatter":{"slug":"spark-execute-tidb","title":"How to use Spark to do OLAP analysis on TiDB","tags":["Spark","TiDB"]},"prevItem":{"title":"How to quickly write data to Elasticsearch using Spark","permalink":"/blog/spark-execute-elasticsearch"},"nextItem":{"title":"How to support Spark StructuredStreaming","permalink":"/blog/spark-structured-streaming"}},"content":"![](https://download.pingcap.com/images/tidb-planet.jpg)\\n\\n[TiDB](https://github.com/pingcap/tidb) is a fusion database product targeting online transaction processing/online analytical processing. Distributed transactions, real-time OLAP and other important features.\\n\\nTiSpark is a product launched by PingCAP to solve the complex OLAP needs of users. It uses the Spark platform and integrates the advantages of TiKV distributed clusters.\\n\\nCompleting OLAP operations with TiSpark directly requires knowledge of Spark and some development work. So, are there some out-of-the-box tools that can help us use TiSpark to complete OLAP analysis on TiDB more quickly?\\n\\nAt present, there is a tool **Seatunnel** in the open source community, the project address [https://github.com/apache/incubator-seatunnel](https://github.com/apache/incubator-seatunnel), which can be based on Spark, Quickly implement TiDB data reading and OLAP analysis based on TiSpark.\\n\\n\\n## Operating TiDB with Seatunnel\\n\\nWe have such a requirement online. Read the website access data of a certain day from TiDB, count the number of visits of each domain name and the status code returned by the service, and finally write the statistical results to another table in TiDB. Let\'s see how Seatunnel implements such a function.\\n\\n### Seatunnel\\n\\n[Seatunnel](https://github.com/apache/incubator-seatunnel) is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plugins that support reading data from TiDB, Kafka, HDFS, Kudu, perform various data processing, and then write the results to TiDB, ClickHouse, Elasticsearch or Kafka.\\n\\n\\n#### Ready to work\\n\\n##### 1. Introduction to TiDB table structure\\n\\n**Input** (table where access logs are stored)\\n\\n```\\nCREATE TABLE access_log (\\n    domain VARCHAR(255),\\n    datetime VARCHAR(63),\\n    remote_addr VARCHAR(63),\\n    http_ver VARCHAR(15),\\n    body_bytes_send INT,\\n    status INT,\\n    request_time FLOAT,\\n    url TEXT\\n)\\n```\\n\\n```\\n+-----------------+--------------+------+------+---------+-------+\\n| Field           | Type         | Null | Key  | Default | Extra |\\n+-----------------+--------------+------+------+---------+-------+\\n| domain          | varchar(255) | YES  |      | NULL    |       |\\n| datetime        | varchar(63)  | YES  |      | NULL    |       |\\n| remote_addr     | varchar(63)  | YES  |      | NULL    |       |\\n| http_ver        | varchar(15)  | YES  |      | NULL    |       |\\n| body_bytes_send | int(11)      | YES  |      | NULL    |       |\\n| status          | int(11)      | YES  |      | NULL    |       |\\n| request_time    | float        | YES  |      | NULL    |       |\\n| url             | text         | YES  |      | NULL    |       |\\n+-----------------+--------------+------+------+---------+-------+\\n```\\n\\n**Output** (table where result data is stored)\\n\\n```\\nCREATE TABLE access_collect (\\n    date VARCHAR(23),\\n    domain VARCHAR(63),\\n    status INT,\\n    hit INT\\n)\\n```\\n\\n```\\n+--------+-------------+------+------+---------+-------+\\n| Field  | Type        | Null | Key  | Default | Extra |\\n+--------+-------------+------+------+---------+-------+\\n| date   | varchar(23) | YES  |      | NULL    |       |\\n| domain | varchar(63) | YES  |      | NULL    |       |\\n| status | int(11)     | YES  |      | NULL    |       |\\n| hit    | int(11)     | YES  |      | NULL    |       |\\n+--------+-------------+------+------+---------+-------+\\n```\\n\\n##### 2. Install Seatunnel\\n\\nAfter we have the input and output tables of TiDB, we need to install Seatunnel. The installation is very simple, and there is no need to configure system environment variables\\n1. Prepare the Spark environment\\n2. Install Seatunnel\\n3. Configure Seatunnel\\n\\nThe following are simple steps, the specific installation can refer to [Quick Start](/docs/quick-start)\\n\\n```\\n# Download and install Spark\\ncd /usr/local\\nwget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz\\ntar -xvf https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz\\nwget\\n# Download and install seatunnel\\nhttps://github.com/InterestingLab/seatunnel/releases/download/v1.2.0/seatunnel-1.2.0.zip\\nunzip seatunnel-1.2.0.zip\\ncd seatunnel-1.2.0\\n\\nvim config/seatunnel-env.sh\\n# Specify the Spark installation path\\nSPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.1.0-bin-hadoop2.7}\\n```\\n\\n\\n### Implement the Seatunnel processing flow\\n\\nWe only need to write a Seatunnel configuration file to read, process, and write data.\\n\\nThe Seatunnel configuration file consists of four parts, `Spark`, `Input`, `Filter` and `Output`. The `Input` part is used to specify the input source of the data, the `Filter` part is used to define various data processing and aggregation, and the `Output` part is responsible for writing the processed data to the specified database or message queue.\\n\\nThe whole processing flow is `Input` -> `Filter` -> `Output`, which constitutes the processing flow (Pipeline) of Seatunnel.\\n\\n> The following is a specific configuration, which is derived from an online practical application, but simplified for demonstration.\\n\\n\\n##### Input (TiDB)\\n\\nThis part of the configuration defines the input source. The following is to read data from a table in TiDB.\\n\\n    input {\\n        tidb {\\n            database = \\"nginx\\"\\n            pre_sql = \\"select * from nginx.access_log\\"\\n            table_name = \\"spark_nginx_input\\"\\n        }\\n    }\\n\\n##### Filter\\n\\nIn the Filter section, here we configure a series of transformations, most of the data analysis requirements are completed in the Filter. Seatunnel provides a wealth of plug-ins enough to meet various data analysis needs. Here we complete the data aggregation operation through the SQL plugin.\\n\\n    filter {\\n        sql {\\n            table_name = \\"spark_nginx_log\\"\\n            sql = \\"select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=\'2019-01-20\' group by domain, status, substring(datetime, 1, 10)\\"\\n        }\\n    }\\n\\n\\n##### Output (TiDB)\\n\\nFinally, we write the processed results to another table in TiDB. TiDB Output is implemented through JDBC\\n\\n    output {\\n        tidb {\\n            url = \\"jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&characterEncoding=utf8\\"\\n            table = \\"access_collect\\"\\n            user = \\"username\\"\\n            password = \\"password\\"\\n            save_mode = \\"append\\"\\n        }\\n    }\\n\\n##### Spark\\n\\nThis part is related to Spark configuration. It mainly configures the resource size required for Spark execution and other Spark configurations.\\n\\nOur TiDB Input plugin is implemented based on TiSpark, which relies on TiKV cluster and Placement Driver (PD). So we need to specify PD node information and TiSpark related configuration `spark.tispark.pd.addresses` and `spark.sql.extensions`.\\n\\n    spark {\\n      spark.app.name = \\"seatunnel-tidb\\"\\n      spark.executor.instances = 2\\n      spark.executor.cores = 1\\n      spark.executor.memory = \\"1g\\"\\n      # Set for TiSpark\\n      spark.tispark.pd.addresses = \\"localhost:2379\\"\\n      spark.sql.extensions = \\"org.apache.spark.sql.TiExtensions\\"\\n    }\\n\\n\\n#### Run Seatunnel\\n\\nWe combine the above four parts into our final configuration file `conf/tidb.conf`\\n\\n```\\nspark {\\n    spark.app.name = \\"seatunnel-tidb\\"\\n    spark.executor.instances = 2\\n    spark.executor.cores = 1\\n    spark.executor.memory = \\"1g\\"\\n    # Set for TiSpark\\n    spark.tispark.pd.addresses = \\"localhost:2379\\"\\n    spark.sql.extensions = \\"org.apache.spark.sql.TiExtensions\\"\\n}\\ninput {\\n    tidb {\\n        database = \\"nginx\\"\\n        pre_sql = \\"select * from nginx.access_log\\"\\n        table_name = \\"spark_table\\"\\n    }\\n}\\nfilter {\\n    sql {\\n        table_name = \\"spark_nginx_log\\"\\n        sql = \\"select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=\'2019-01-20\' group by domain, status, substring(datetime, 1, 10)\\"\\n    }\\n}\\noutput {\\n    tidb {\\n        url = \\"jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&characterEncoding=utf8\\"\\n        table = \\"access_collect\\"\\n        user = \\"username\\"\\n        password = \\"password\\"\\n        save_mode = \\"append\\"\\n    }\\n}\\n```\\n\\nExecute the command, specify the configuration file, and run Seatunnel to implement our data processing logic.\\n\\n* Local\\n\\n> ./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode client --master \'local[2]\'\\n\\n* yarn-client\\n\\n> ./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode client --master yarn\\n\\n* yarn-cluster\\n\\n> ./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode cluster -master yarn\\n\\nIf it is a local test and verification logic, you can use the local mode (Local). Generally, in the production environment, the `yarn-client` or `yarn-cluster` mode is used.\\n\\n#### test result\\n\\n```\\nmysql> select * from access_collect;\\n+------------+--------+--------+------+\\n| date       | domain | status | hit  |\\n+------------+--------+--------+------+\\n| 2019-01-20 | b.com  |    200 |   63 |\\n| 2019-01-20 | a.com  |    200 |   85 |\\n+------------+--------+--------+------+\\n2 rows in set (0.21 sec)\\n```\\n\\n\\n\\n## Conclusion\\n\\nIn this article, we introduced how to use Seatunnel to read data from TiDB, do simple data processing and write it to another table in TiDB. Data can be imported quickly with only one configuration file without writing any code.\\n\\nIn addition to supporting TiDB data sources, Seatunnel also supports Elasticsearch, Kafka, Kudu, ClickHouse and other data sources.\\n\\n**At the same time, we are developing an important function, which is to use the transaction features of TiDB in Seatunnel to realize streaming data processing from Kafka to TiDB, and support Exactly-Once data from end (Kafka) to end (TiDB). consistency. **\\n\\nIf you want to know more functions and cases of Seatunnel combined with TiDB, ClickHouse, Elasticsearch and Kafka, you can go directly to the official website [https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n\\n## Contract us\\n* Mailing list : **dev@seatunnel.apache.org**. Send anything to `dev-subscribe@seatunnel.apache.org` and subscribe to the mailing list according to the replies.\\n* Slack: Send a `Request to join SeaTunnel slack` email to the mailing list (`dev@seatunnel.apache.org`), and we will invite you to join (please make sure you are registered with Slack before doing so).\\n* [bilibili B station video](https://space.bilibili.com/1542095008)\\n\\n-- Power by [InterestingLab](https://github.com/InterestingLab)"},{"id":"spark-structured-streaming","metadata":{"permalink":"/blog/spark-structured-streaming","editUrl":"https://github.com/apache/incubator-seatunnel-website/edit/main/blog/2021-12-30-spark-structured-streaming.md","source":"@site/blog/2021-12-30-spark-structured-streaming.md","title":"How to support Spark StructuredStreaming","description":"Foreword","date":"2021-12-30T00:00:00.000Z","formattedDate":"December 30, 2021","tags":[{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"StructuredStreaming","permalink":"/blog/tags/structured-streaming"}],"readingTime":7.8,"truncated":false,"authors":[],"frontMatter":{"slug":"spark-structured-streaming","title":"How to support Spark StructuredStreaming","tags":["Spark","StructuredStreaming"]},"prevItem":{"title":"How to use Spark to do OLAP analysis on TiDB","permalink":"/blog/spark-execute-tidb"}},"content":"### Foreword\\n\\nStructuredStreaming is a newly opened module after Spark 2.0. Compared with SparkStreaming, it has some prominent advantages:<br/> &emsp;&emsp;First, it can achieve lower latency;<br/>\\n&emsp;&emsp;Second, real-time aggregation can be done, such as real-time calculation of the total sales of each commodity every day;<br/>\\n&emsp;&emsp;Third, you can do the association between streams, for example, to calculate the click rate of an advertisement, you need to associate the exposure record of the advertisement with the click record. <br/>\\nThe above points may be cumbersome or difficult to implement if using SparkStreaming, but it will be easier to implement using StructuredStreaming.\\n### How to use StructuredStreaming\\nMaybe you have not studied StructuredStreaming in detail, but found that StructuredStreaming can solve your needs very well. How to quickly use StructuredStreaming to solve your needs? Currently there is a tool **Seatunnel** in the community, the project address: [https://github.com/apache/incubator-seatunnel](https://github.com/apache/incubator-seatunnel) ,\\nIt can help you use StructuredStreaming to complete your needs efficiently and at low cost.\\n\\n### Seatunnel\\n\\nSeatunnel is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plug-ins, supports reading data from Kafka, HDFS, Kudu, performs various data processing, and writes the results to ClickHouse, Elasticsearch or Kafka\\n\\n### Ready to work\\n\\nFirst we need to install Seatunnel, the installation is very simple, no need to configure system environment variables\\n\\n1. Prepare the Spark environment\\n2. Install Seatunnel\\n3. Configure Seatunnel\\n\\nThe following are simple steps, the specific installation can refer to [Quick Start](/docs/quick-start)\\n\\n```\\ncd /usr/local\\nwget https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\ntar -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\nwget https://github.com/InterestingLab/seatunnel/releases/download/v1.3.0/seatunnel-1.3.0.zip\\nunzip seatunnel-1.3.0.zip\\ncd seatunnel-1.3.0\\n\\nvim config/seatunnel-env.sh\\n# Specify the Spark installation path\\nSPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.2.0-bin-hadoop2.7}\\n```\\n\\n### Seatunnel Pipeline\\n\\nWe only need to write a configuration file of Seatunnel Pipeline to complete the data import.\\n\\nThe configuration file includes four parts, namely Spark, Input, filter and Output.\\n\\n#### Spark\\n\\nThis part is the related configuration of Spark, which mainly configures the resource size required for Spark execution.\\n\\n```\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n```\\n\\n#### Input\\n\\nBelow is an example of reading data from kafka\\n\\n```\\nkafkaStream {\\n    topics = \\"seatunnel\\"\\n    consumer.bootstrap.servers = \\"localhost:9092\\"\\n    schema = \\"{\\\\\\"name\\\\\\":\\\\\\"string\\\\\\",\\\\\\"age\\\\\\":\\\\\\"integer\\\\\\",\\\\\\"addrs\\\\\\":{\\\\\\"country\\\\\\":\\\\\\"string\\\\\\",\\\\\\"city\\\\\\":\\\\\\"string\\\\\\"}}\\"\\n}\\n```\\n\\nThrough the above configuration, the data in kafka can be read. Topics is the topic of kafka to be subscribed to. Subscribing to multiple topics at the same time can be separated by commas. Consumer.bootstrap.servers is the list of Kafka servers, and schema is optional. Because the value read by StructuredStreaming from kafka (official fixed field value) is of binary type, see http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\\nBut if you are sure that the data in your kafka is a json string, you can specify the schema, and the input plugin will parse it according to the schema you specify\\n\\n#### Filter\\n\\nHere is a simple filter example\\n\\n```\\nfilter{\\n    sql{\\n        table_name = \\"student\\"\\n        sql = \\"select name,age from student\\"\\n    }\\n}\\n```\\n`table_name` is the registered temporary table name for easy use in the following sql\\n\\n#### Output\\n\\nThe processed data is output, assuming that our output is also kafka\\n\\n```\\noutput{\\n    kafka {\\n        topic = \\"seatunnel\\"\\n        producer.bootstrap.servers = \\"localhost:9092\\"\\n        streaming_output_mode = \\"update\\"\\n        checkpointLocation = \\"/your/path\\"\\n    }\\n}\\n```\\n\\n`topic` is the topic you want to output, `producer.bootstrap.servers` is a list of kafka clusters, `streaming_output_mode` is an output mode parameter of StructuredStreaming, there are three types of `append|update|complete`, for details, see the documentation http: //spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes\\n\\n`checkpointLocation` is the checkpoint path of StructuredStreaming. If configured, this directory will store the running information of the program. For example, if the program exits and restarts, it will continue to consume the last offset.\\n\\n### Scenario Analysis\\n\\nThe above is a simple example. Next, we will introduce a slightly more complex business scenario.\\n\\n#### Scenario 1: Real-time aggregation scenario\\n\\nSuppose there is now a mall with 10 kinds of products on it, and now it is necessary to find the daily sales of each product in real time, and even to find the number of buyers of each product (not very precise).\\nThe huge advantage of this is that massive data can be aggregated during real-time processing, and there is no need to write data into the data warehouse first, and then run offline scheduled tasks for aggregation.\\nIt is still very convenient to operate.\\n\\nThe data of kafka is as follows\\n\\n```\\n{\\"good_id\\":\\"abc\\",\\"price\\":300,\\"user_id\\":123456,\\"time\\":1553216320}\\n```\\n\\nSo how do we use Seatunnel to fulfill this requirement, of course, we only need to configure it.\\n\\n```\\n#The configuration in spark is configured according to business requirements\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n\\n#configure input\\ninput {\\n    kafkaStream {\\n        topics = \\"good_topic\\"\\n        consumer.bootstrap.servers = \\"localhost:9092\\"\\n        schema = \\"{\\\\\\"good_id\\\\\\":\\\\\\"string\\\\\\",\\\\\\"price\\\\\\":\\\\\\"integer\\\\\\",\\\\\\"user_id\\\\\\":\\\\\\"Long\\\\\\",\\\\\\"time\\\\\\":\\\\\\"Long\\\\\\"}\\"\\n    }\\n}\\n\\n#configure filter    \\nfilter {\\n    \\n    #When the program is doing aggregation, it will internally store the aggregation state of the program since startup, which will lead to OOM over time. If the watermark is set, the program will automatically clean up the state other than the watermark.\\n    #Here means use the ts field to set the watermark, the limit is 1 day\\n\\n    Watermark {\\n        time_field = \\"time\\"\\n        time_type = \\"UNIX\\"              #UNIX represents a timestamp with a time field of 10, and other types can be found in the plugin documentation for details.\\n        time_pattern = \\"yyyy-MM-dd\\"     #The reason why the ts is assigned to the day is because the daily sales are sought, if the hourly sales are sought, the hour can be assigned `yyyy-MM-dd HH`\\n        delay_threshold = \\"1 day\\"\\n        watermark_field = \\"ts\\"          #After setting the watermark, a new field will be added, `ts` is the name of this field\\n    }\\n    \\n    #The reason for group by ts is to make the watermark take effect, approx_count_distinct is an estimate, not an exact count_distinct\\n    sql {\\n        table_name = \\"good_table_2\\"\\n        sql = \\"select good_id,sum(price) total,\\tapprox_count_distinct(user_id) person from good_table_2 group by ts,good_id\\"\\n    }\\n}\\n\\n#Next we choose to output the results to Kafka in real time\\noutput{\\n    kafka {\\n        topic = \\"seatunnel\\"\\n        producer.bootstrap.servers = \\"localhost:9092\\"\\n        streaming_output_mode = \\"update\\"\\n        checkpointLocation = \\"/your/path\\"\\n    }\\n}\\n\\n```\\nThe above configuration is complete, start Seatunnel, and you can get the results you want.\\n\\n#### Scenario 2: Multiple stream association scenarios\\n\\nSuppose you have placed an advertisement on a certain platform, and now you need to calculate the CTR (click-through rate) of each advertisement in real time. The data comes from two topics, one is the advertisement exposure log, and the other is the advertisement click log.\\nAt this point, we need to associate the two stream data together for calculation, and Seatunnel also supports this function recently, let\'s take a look at how to do it:\\n\\n\\nClick on topic data format\\n\\n```\\n{\\"ad_id\\":\\"abc\\",\\"click_time\\":1553216320,\\"user_id\\":12345}\\n\\n```\\n\\nExposure topic data format\\n\\n```\\n{\\"ad_id\\":\\"abc\\",\\"show_time\\":1553216220,\\"user_id\\":12345}\\n\\n```\\n\\n```\\n#The configuration in spark is configured according to business requirements\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n\\n#configure input\\ninput {\\n    \\n    kafkaStream {\\n        topics = \\"click_topic\\"\\n        consumer.bootstrap.servers = \\"localhost:9092\\"\\n        schema = \\"{\\\\\\"ad_id\\\\\\":\\\\\\"string\\\\\\",\\\\\\"user_id\\\\\\":\\\\\\"Long\\\\\\",\\\\\\"click_time\\\\\\":\\\\\\"Long\\\\\\"}\\"\\n        table_name = \\"click_table\\"\\n    }\\n    \\n    kafkaStream {\\n        topics = \\"show_topic\\"\\n        consumer.bootstrap.servers = \\"localhost:9092\\"\\n        schema = \\"{\\\\\\"ad_id\\\\\\":\\\\\\"string\\\\\\",\\\\\\"user_id\\\\\\":\\\\\\"Long\\\\\\",\\\\\\"show_time\\\\\\":\\\\\\"Long\\\\\\"}\\"\\n        table_name = \\"show_table\\"\\n    }\\n}\\n\\nfilter {\\n    \\n    #Left association right table must set watermark\\n    #Right off left and right tables must set watermark\\n    #http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#inner-joins-with-optional-watermarking\\n    Watermark {\\n              source_table_name = \\"click_table\\" #Here you can specify to add a watermark to a temporary table. If you don\'t specify it, it will be the first one in the input.\\n              time_field = \\"time\\"\\n              time_type = \\"UNIX\\"               \\n              delay_threshold = \\"3 hours\\"\\n              watermark_field = \\"ts\\" \\n              result_table_name = \\"click_table_watermark\\" #After adding the watermark, it can be registered as a temporary table, which is convenient for subsequent use in sql\\n    }\\n    \\n    Watermark {\\n                source_table_name = \\"show_table\\" \\n                time_field = \\"time\\"\\n                time_type = \\"UNIX\\"               \\n                delay_threshold = \\"2 hours\\"\\n                watermark_field = \\"ts\\" \\n                result_table_name = \\"show_table_watermark\\" \\n     }\\n    \\n    \\n    sql {\\n        table_name = \\"show_table_watermark\\"\\n        sql = \\"select a.ad_id,count(b.user_id)/count(a.user_id) ctr from show_table_watermark as a left join click_table_watermark as b on a.ad_id = b.ad_id and a.user_id = b.user_id \\"\\n    }\\n    \\n}\\n\\n#Next we choose to output the results to Kafka in real time\\noutput {\\n    kafka {\\n        topic = \\"seatunnel\\"\\n        producer.bootstrap.servers = \\"localhost:9092\\"\\n        streaming_output_mode = \\"append\\" #Stream association only supports append mode\\n        checkpointLocation = \\"/your/path\\"\\n    }\\n}\\n```\\nThrough configuration, the case of stream association is also completed here.\\n\\n### Conclusion\\nThrough configuration, you can quickly use StructuredStreaming for real-time data processing, but you still need to understand some concepts of StructuredStreaming, such as the watermark mechanism, and the output mode of the program.\\n\\nFinally, Seatunnel also supports spark streaming and spark batching of course.\\nIf you are also interested in these two, you can read our previous article \\"[How to quickly import data from Hive into ClickHouse](2021-12-30-hive-to-clickhouse.md)\\",\\n\\"[Excellent data engineer, how to use Spark to do OLAP analysis on TiDB] (2021-12-30-spark-execute-tidb.md)\\",\\n\\"[How to use Spark to quickly write data to Elasticsearch] (2021-12-30-spark-execute-elasticsearch.md)\\"\\n\\nIf you want to know more functions and cases of Seatunnel combined with HBase, ClickHouse, Elasticsearch, Kafka, MySQL and other data sources, you can go directly to the official website [https://seatunnel.apache.org/](https://seatunnel.apache. org/)\\n\\n## \u8054\u7cfb\u6211\u4eec\\n* Mailing list : **dev@seatunnel.apache.org**. Send anything to `dev-subscribe@seatunnel.apache.org` and subscribe to the mailing list according to the replies.\\n* Slack: Send a `Request to join SeaTunnel slack` email to the mailing list (`dev@seatunnel.apache.org`), and we will invite you to join (please make sure you are registered with Slack before doing so).\\n* [bilibili B station video](https://space.bilibili.com/1542095008)"}]}')}}]);